{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01ecd8c",
   "metadata": {},
   "source": [
    "# LOCALTRIAGE - Weekly Insights Report\n",
    "\n",
    "This notebook generates weekly product issue insights from support tickets.\n",
    "It performs:\n",
    "1. Trend analysis of ticket categories and priorities\n",
    "2. Emerging issue detection using topic clustering\n",
    "3. Resolution time analysis\n",
    "4. Actionable recommendations generation\n",
    "\n",
    "Run this notebook weekly (or schedule via cron/Airflow) to produce insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Configuration\n",
    "os.environ['DB_HOST'] = 'localhost'\n",
    "os.environ['DB_PORT'] = '5432'\n",
    "os.environ['DB_NAME'] = 'localtriage'\n",
    "os.environ['DB_USER'] = 'postgres'\n",
    "os.environ['DB_PASSWORD'] = 'postgres'\n",
    "\n",
    "# Report parameters\n",
    "REPORT_WEEKS = 1  # Number of weeks to analyze\n",
    "REPORT_DATE = datetime.now()\n",
    "START_DATE = REPORT_DATE - timedelta(weeks=REPORT_WEEKS)\n",
    "\n",
    "print(f\"Report Period: {START_DATE.strftime('%Y-%m-%d')} to {REPORT_DATE.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981cc4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingestion.ingest import DatabaseConnection\n",
    "\n",
    "# Fetch all tickets in the reporting period\n",
    "with DatabaseConnection() as conn:\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            subject,\n",
    "            body,\n",
    "            category,\n",
    "            priority,\n",
    "            status,\n",
    "            customer_email,\n",
    "            created_at,\n",
    "            resolved_at\n",
    "        FROM tickets\n",
    "        WHERE created_at >= %s AND created_at <= %s\n",
    "        ORDER BY created_at;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, conn, params=(START_DATE, REPORT_DATE))\n",
    "\n",
    "print(f\"Loaded {len(df)} tickets for analysis\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffe78b",
   "metadata": {},
   "source": [
    "## 1. Volume & Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily volume trend\n",
    "df['date'] = pd.to_datetime(df['created_at']).dt.date\n",
    "daily_volume = df.groupby('date').size().reset_index(name='count')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Volume over time\n",
    "axes[0].plot(daily_volume['date'], daily_volume['count'], marker='o', linewidth=2)\n",
    "axes[0].set_title('Daily Ticket Volume', fontsize=14)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Tickets')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Week-over-week comparison (if enough data)\n",
    "if len(daily_volume) >= 7:\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.day_name()\n",
    "    dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    dow_counts = df['day_of_week'].value_counts().reindex(dow_order)\n",
    "    axes[1].bar(dow_counts.index, dow_counts.values, color='steelblue')\n",
    "    axes[1].set_title('Tickets by Day of Week', fontsize=14)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Insufficient data for\\nday-of-week analysis', \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats\n",
    "print(f\"\\n[CHART] Volume Summary:\")\n",
    "print(f\"  Total tickets: {len(df)}\")\n",
    "print(f\"  Daily average: {len(df) / max(len(daily_volume), 1):.1f}\")\n",
    "print(f\"  Peak day: {daily_volume.loc[daily_volume['count'].idxmax(), 'date']} ({daily_volume['count'].max()} tickets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Category pie chart\n",
    "category_counts = df['category'].value_counts()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "axes[0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[0].set_title('Ticket Distribution by Category', fontsize=14)\n",
    "\n",
    "# Priority distribution\n",
    "priority_order = ['P1', 'P2', 'P3', 'P4']\n",
    "priority_colors = {'P1': '#dc3545', 'P2': '#fd7e14', 'P3': '#ffc107', 'P4': '#28a745'}\n",
    "priority_counts = df['priority'].value_counts().reindex(priority_order).fillna(0)\n",
    "bars = axes[1].bar(priority_counts.index, priority_counts.values, \n",
    "                   color=[priority_colors.get(p, 'gray') for p in priority_counts.index])\n",
    "axes[1].set_title('Ticket Distribution by Priority', fontsize=14)\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[TREND] Category Breakdown:\")\n",
    "for cat, count in category_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {cat}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1580d5",
   "metadata": {},
   "source": [
    "## 2. Emerging Issue Detection\n",
    "\n",
    "Use topic clustering to identify emerging patterns in tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584083a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine subject and body for clustering\n",
    "df['text'] = df['subject'].fillna('') + ' ' + df['body'].fillna('')\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "if len(df) >= 5:  # Need minimum samples\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "else:\n",
    "    print(\"Insufficient data for clustering analysis\")\n",
    "    tfidf_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96150d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms(cluster_centers, feature_names, n_terms=10):\n",
    "    \"\"\"Extract top terms for each cluster\"\"\"\n",
    "    top_terms = {}\n",
    "    for i, center in enumerate(cluster_centers):\n",
    "        top_indices = center.argsort()[-n_terms:][::-1]\n",
    "        terms = [feature_names[idx] for idx in top_indices]\n",
    "        top_terms[i] = terms\n",
    "    return top_terms\n",
    "\n",
    "if tfidf_matrix is not None and tfidf_matrix.shape[0] >= 5:\n",
    "    # Determine optimal cluster count (simplified)\n",
    "    n_clusters = min(5, len(df) // 3)\n",
    "    n_clusters = max(2, n_clusters)\n",
    "    \n",
    "    # K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
    "    \n",
    "    # Get top terms per cluster\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    cluster_terms = get_top_terms(kmeans.cluster_centers_, feature_names)\n",
    "    \n",
    "    print(f\"\\n[SEARCH] Identified {n_clusters} Topic Clusters:\\n\")\n",
    "    for cluster_id, terms in cluster_terms.items():\n",
    "        count = (df['cluster'] == cluster_id).sum()\n",
    "        print(f\"Cluster {cluster_id + 1} ({count} tickets):\")\n",
    "        print(f\"  Keywords: {', '.join(terms[:5])}\")\n",
    "        \n",
    "        # Sample ticket from cluster\n",
    "        sample = df[df['cluster'] == cluster_id].iloc[0]\n",
    "        print(f\"  Example: \\\"{sample['subject'][:60]}...\\\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters with PCA\n",
    "if tfidf_matrix is not None and tfidf_matrix.shape[0] >= 5:\n",
    "    # Reduce to 2D\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    coords = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(coords[:, 0], coords[:, 1], \n",
    "                         c=df['cluster'], cmap='Set2', \n",
    "                         alpha=0.7, s=100)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Ticket Topic Clusters (PCA Visualization)', fontsize=14)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a76f9",
   "metadata": {},
   "source": [
    "## 3. Resolution Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab00843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate resolution times\n",
    "resolved = df[df['resolved_at'].notna()].copy()\n",
    "\n",
    "if len(resolved) > 0:\n",
    "    resolved['resolution_hours'] = (\n",
    "        pd.to_datetime(resolved['resolved_at']) - \n",
    "        pd.to_datetime(resolved['created_at'])\n",
    "    ).dt.total_seconds() / 3600\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Resolution time distribution\n",
    "    axes[0].hist(resolved['resolution_hours'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(resolved['resolution_hours'].median(), color='red', \n",
    "                    linestyle='--', label=f'Median: {resolved[\"resolution_hours\"].median():.1f}h')\n",
    "    axes[0].set_title('Resolution Time Distribution', fontsize=14)\n",
    "    axes[0].set_xlabel('Hours')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Resolution time by priority\n",
    "    priority_resolution = resolved.groupby('priority')['resolution_hours'].median().reindex(priority_order)\n",
    "    bars = axes[1].bar(priority_resolution.index, priority_resolution.values,\n",
    "                       color=[priority_colors.get(p, 'gray') for p in priority_resolution.index])\n",
    "    axes[1].set_title('Median Resolution Time by Priority', fontsize=14)\n",
    "    axes[1].set_ylabel('Hours')\n",
    "    \n",
    "    # SLA reference lines\n",
    "    sla_targets = {'P1': 4, 'P2': 24, 'P3': 72, 'P4': 168}\n",
    "    for i, (p, target) in enumerate(sla_targets.items()):\n",
    "        if p in priority_resolution.index:\n",
    "            axes[1].axhline(target, color=priority_colors[p], linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nResolution Time Summary:\")\n",
    "    print(f\"  Total resolved: {len(resolved)} ({len(resolved)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Median time: {resolved['resolution_hours'].median():.1f} hours\")\n",
    "    print(f\"  Mean time: {resolved['resolution_hours'].mean():.1f} hours\")\n",
    "    print(f\"  90th percentile: {resolved['resolution_hours'].quantile(0.9):.1f} hours\")\n",
    "else:\n",
    "    print(\"No resolved tickets in the reporting period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a537a5f",
   "metadata": {},
   "source": [
    "## 4. Key Insights & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(df):\n",
    "    \"\"\"Generate actionable insights from ticket data\"\"\"\n",
    "    insights = []\n",
    "    \n",
    "    # High priority ticket analysis\n",
    "    p1_count = (df['priority'] == 'P1').sum()\n",
    "    p1_pct = p1_count / len(df) * 100 if len(df) > 0 else 0\n",
    "    if p1_pct > 10:\n",
    "        insights.append({\n",
    "            'type': 'warning',\n",
    "            'title': 'High P1 Ticket Volume',\n",
    "            'detail': f'{p1_count} P1 tickets ({p1_pct:.1f}%) - investigate root cause',\n",
    "            'action': 'Review P1 tickets for common patterns; consider escalation procedures'\n",
    "        })\n",
    "    \n",
    "    # Category concentration\n",
    "    top_category = df['category'].value_counts().idxmax() if len(df) > 0 else None\n",
    "    top_cat_pct = df['category'].value_counts().max() / len(df) * 100 if len(df) > 0 else 0\n",
    "    if top_cat_pct > 40:\n",
    "        insights.append({\n",
    "            'type': 'trend',\n",
    "            'title': f'{top_category} Category Dominance',\n",
    "            'detail': f'{top_cat_pct:.1f}% of tickets are {top_category}-related',\n",
    "            'action': f'Investigate {top_category.lower()} issues; consider dedicated support or product fixes'\n",
    "        })\n",
    "    \n",
    "    # Cluster-based insights\n",
    "    if 'cluster' in df.columns:\n",
    "        largest_cluster = df['cluster'].value_counts().idxmax()\n",
    "        cluster_size = df['cluster'].value_counts().max()\n",
    "        if cluster_size / len(df) > 0.3:\n",
    "            # Get sample keywords for this cluster\n",
    "            cluster_samples = df[df['cluster'] == largest_cluster]['subject'].head(3).tolist()\n",
    "            insights.append({\n",
    "                'type': 'emerging',\n",
    "                'title': 'Emerging Issue Pattern Detected',\n",
    "                'detail': f'Cluster of {cluster_size} similar tickets identified',\n",
    "                'action': f'Review tickets like: \"{cluster_samples[0][:50]}...\"'\n",
    "            })\n",
    "    \n",
    "    return insights\n",
    "\n",
    "insights = generate_insights(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[REPORT] WEEKLY INSIGHTS REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Period: {START_DATE.strftime('%Y-%m-%d')} to {REPORT_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total Tickets Analyzed: {len(df)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if insights:\n",
    "    print(\"\\n[TARGET] KEY INSIGHTS:\\n\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        icon = '[WARN]' if insight['type'] == 'warning' else '[TREND]' if insight['type'] == 'trend' else '[NEW]'\n",
    "        print(f\"{icon} {i}. {insight['title']}\")\n",
    "        print(f\"   {insight['detail']}\")\n",
    "        print(f\"   â†’ Action: {insight['action']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n[DONE] No significant issues detected this week.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to file\n",
    "report_dir = project_root / 'reports'\n",
    "report_dir.mkdir(exist_ok=True)\n",
    "\n",
    "report_filename = f\"weekly_insights_{REPORT_DATE.strftime('%Y%m%d')}.md\"\n",
    "report_path = report_dir / report_filename\n",
    "\n",
    "report_content = f\"\"\"# Weekly Support Insights Report\n",
    "\n",
    "**Period:** {START_DATE.strftime('%Y-%m-%d')} to {REPORT_DATE.strftime('%Y-%m-%d')}  \n",
    "**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "- **Total Tickets:** {len(df)}\n",
    "- **Daily Average:** {len(df) / max((REPORT_DATE - START_DATE).days, 1):.1f}\n",
    "- **Top Category:** {df['category'].value_counts().idxmax() if len(df) > 0 else 'N/A'}\n",
    "- **P1 Tickets:** {(df['priority'] == 'P1').sum()}\n",
    "\n",
    "## Category Distribution\n",
    "\n",
    "| Category | Count | Percentage |\n",
    "|----------|-------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for cat, count in df['category'].value_counts().items():\n",
    "    pct = count / len(df) * 100 if len(df) > 0 else 0\n",
    "    report_content += f\"| {cat} | {count} | {pct:.1f}% |\\n\"\n",
    "\n",
    "report_content += \"\\n## Key Insights\\n\\n\"\n",
    "\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    report_content += f\"### {i}. {insight['title']}\\n\\n\"\n",
    "    report_content += f\"{insight['detail']}\\n\\n\"\n",
    "    report_content += f\"**Recommended Action:** {insight['action']}\\n\\n\"\n",
    "\n",
    "if not insights:\n",
    "    report_content += \"No significant issues detected this week.\\n\\n\"\n",
    "\n",
    "report_content += \"---\\n*Report generated by LOCALTRIAGE Insights Engine*\\n\"\n",
    "\n",
    "report_path.write_text(report_content)\n",
    "print(f\"\\n[FILE] Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340147a8",
   "metadata": {},
   "source": [
    "## 5. Save Metrics to Database\n",
    "\n",
    "Persist weekly metrics for historical tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87929a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated metrics to database\n",
    "if len(df) > 0:\n",
    "    metrics = {\n",
    "        'report_date': REPORT_DATE.date(),\n",
    "        'period_start': START_DATE.date(),\n",
    "        'period_end': REPORT_DATE.date(),\n",
    "        'total_tickets': int(len(df)),\n",
    "        'p1_count': int((df['priority'] == 'P1').sum()),\n",
    "        'p2_count': int((df['priority'] == 'P2').sum()),\n",
    "        'p3_count': int((df['priority'] == 'P3').sum()),\n",
    "        'p4_count': int((df['priority'] == 'P4').sum()),\n",
    "        'top_category': df['category'].value_counts().idxmax(),\n",
    "        'insight_count': len(insights)\n",
    "    }\n",
    "    \n",
    "    print(\"Weekly Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save to database (if table exists)\n",
    "    try:\n",
    "        with DatabaseConnection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO daily_metrics (\n",
    "                    metric_date, total_tickets, p1_tickets, p2_tickets, p3_tickets, p4_tickets\n",
    "                ) VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (metric_date) DO UPDATE SET\n",
    "                    total_tickets = EXCLUDED.total_tickets,\n",
    "                    p1_tickets = EXCLUDED.p1_tickets,\n",
    "                    p2_tickets = EXCLUDED.p2_tickets,\n",
    "                    p3_tickets = EXCLUDED.p3_tickets,\n",
    "                    p4_tickets = EXCLUDED.p4_tickets;\n",
    "            \"\"\", (\n",
    "                metrics['report_date'],\n",
    "                metrics['total_tickets'],\n",
    "                metrics['p1_count'],\n",
    "                metrics['p2_count'],\n",
    "                metrics['p3_count'],\n",
    "                metrics['p4_count']\n",
    "            ))\n",
    "            conn.commit()\n",
    "            print(\"\\n[DONE] Metrics saved to database.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[WARN] Could not save to database: {e}\")\n",
    "else:\n",
    "    print(\"No data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90bb7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. [DONE] Analyzed ticket volume and trends\n",
    "2. [DONE] Generated category and priority distributions\n",
    "3. [DONE] Detected emerging issue patterns via clustering\n",
    "4. [DONE] Calculated resolution time metrics\n",
    "5. [DONE] Produced actionable insights\n",
    "6. [DONE] Exported report to Markdown file\n",
    "7. [DONE] Persisted metrics to database\n",
    "\n",
    "**Next Steps:**\n",
    "- Review insights with product team\n",
    "- Create tickets for identified issues\n",
    "- Update KB articles if gaps detected\n",
    "- Schedule this notebook for weekly execution"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}